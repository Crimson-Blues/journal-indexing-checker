# -*- coding: utf-8 -*-
"""Productos_Indexados.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V4y2FF4ra6LUXr01bbSGwXEVZkQFepmx

CELDA DE PREPARACIÓN: (**EJECUTAR UNA SOLA VEZ**)
1. Ejecuta la siguiente celda:

  $\cdot$ Haciendo click en el botón de _play_ en la esquina superior izquierda de la celda.
  
  $\cdot$ O presionando _ctrl+enter_ al estar al interior de la celda.

1.1 Asegúrate de otorgar los permisos requeridos para el acceso de _Google Drive_

Para mayor información sobre el procedimiento revisa la guía de usuario:
https://docs.google.com/document/d/13xuEsGxLLaxRKaCh0xqEwdMVgZbr_86JJ_Zv-i2WCdY/edit?usp=sharing
"""

#Install libraries
print("Instalando librerias...")
print("Actualizando libreria Gdown...")
!pip install --upgrade gdown > /dev/null
print("Instalando libreria Chromium...")
!apt install chromium-chromedriver > /dev/null
print("Instalando libreria Selenium...")
!pip3 install selenium > /dev/null
!pip install google-colab-selenium > /dev/null

#Library imports
import requests
import gspread
from google.auth import default
import pandas as pd
import time
import gdown
import numpy
import os

# Authenticate and connect to Google Sheets
from google.colab import auth
auth.authenticate_user()
creds, _ = default()
gc = gspread.authorize(creds)
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload


from bs4 import BeautifulSoup

#Regular expressions
import re

#Selenium Library
import google_colab_selenium as gs

#Libraries for web-scraping
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import google_colab_selenium as gs
from selenium.common.exceptions import TimeoutException
from IPython.display import Image, display

#Default dictionaries
from collections import defaultdict

# Create the Drive service
drive_service = build("drive", "v3")


# Create the Drive service
drive_service = build("drive", "v3")

#Download MJL files if needed

if not os.path.exists('/content/SCIE.csv'):
  safe_download("https://drive.google.com/uc?id=1V0XJH1_fgJ_7cDngUkNwzaVCbFiFzQzj", '/content/SCIE.csv', quiet=False)
else:
  print("Archivo SCIE ya hallado")

if not os.path.exists('/content/ESCI.csv'):
  safe_download("https://drive.google.com/uc?id=1oi2Rs4FDGWBRkx2MX-V8jzf9wJwbyY9t", '/content/ESCI.csv', quiet=False)
else:
  print("Archivo ESCI ya hallado")

if not os.path.exists('/content/AHCI.csv'):
  safe_download("https://drive.google.com/uc?id=1B0w4kw-msuhtOY9HztHfESwGbeA7iFqn", '/content/AHCI.csv', quiet=False)
else:
  print("Archivo AHCI ya hallado")

if not os.path.exists('/content/SSCI.csv'):
  safe_download("https://drive.google.com/uc?id=1FzIZLKNArFgELYVdRzUtci4NAQ8uiSqQ", '/content/SSCI.csv', quiet=False)
else:
  print("Archivo SSCI ya hallado")

#Global variables for MJL
scie_df = pd.read_csv("/content/SCIE.csv")
esci_df = pd.read_csv("/content/ESCI.csv")
ahci_df = pd.read_csv("/content/AHCI.csv")
scii_df = pd.read_csv("/content/SSCI.csv")
mjl_df = pd.concat([scie_df, esci_df, ahci_df, scii_df], ignore_index=True)


#Default dictionary to save Publindex known results
publindex_found = defaultdict(list)

#FUNCTIONS

#Safe download from Google Drive, managing error messages
def safe_download(url: str, output: str = '/content/new_file.txt', retries: int = 3, delay: int = 3, quiet: bool = False):
  for attempt in range(1, retries + 1):
      try:
          print(f"Intento #{attempt} de descarga...")
          gdown.download(url, output, quiet=quiet)
          print("Descarga exitosa")
          return True

      except PermissionError:
          print("Error: No tienes permiso para acceder al archivo")
          break  #No need to retry

      except requests.exceptions.RequestException as e:
          print(f"Error de Red: {e}")
          if attempt < retries:
              print(f"Reintentando en {delay} segundos...")
              time.sleep(delay)
          else:
              print("Eror de red: Máximo numéro de intentos alcanzados.")

      except OSError as e:
          print(f"Error al guardar el archivo localmente: {e}")
          break  # Local file system problem, don't retry

      except Exception as e:
          print(f"Error inesperado {e}")
          break

  return False  # If it exits the loop without returning True

#Gets urls from the excel file, checks indexing and saves results
def search_indexed_journals(url):
  match= re.search(r'(?<=/d/)[a-zA-Z0-9-_]+(?=/edit)',  url)

  if match:
    file_id = match.group()
  else:
    print("URL no válido")
    return

  output_file = "/content/Grupos.xlsx"

  safe_download(f"https://drive.google.com/uc?id={file_id}", output_file)

  #Excel sheet containing the gruplac links
  links_sheets = pd.read_excel(output_file,
                           sheet_name = 1)

  #Eliminates empty rows
  gruplac_links = links_sheets.iloc[:,2].dropna()
  codes = links_sheets.iloc[:,3].dropna()


  #Regex expression for gruplac link general form
  gruplac_pattern = r'https?://(?:www\.)?scienti\.minciencias\.gov\.co/gruplac/jsp/visualiza/visualizagr\.jsp\?nro=\d+'

  gruplac_links = gruplac_links.astype(str).str.findall(gruplac_pattern)
  gruplac_links = gruplac_links.explode().dropna()

  codes = codes.astype(str).str.findall(r'COL\d+')
  codes = codes.explode().dropna()
  df_codes = codes.to_frame(name = 'Códigos')
  df_codes.reset_index(drop=True, inplace=True)
  df_codes.index = df_codes.index + 1
  print("Códigos de grupo hallados: ")
  display(df_codes)

  i = 0
  results = []
  for group in gruplac_links:
    results.clear()
    print("Código de grupo: " + str(codes.iloc[i]))
    print("URL de Gruplac: " + group)
    issn_list, articles = issn_list_from_gruplac(group)
    df_results = pd.DataFrame(articles, columns=["Artículos Publicados"])

    print("Verificando indexación en Publindex...")
    df_results['Publindex'] = check_publindex_indexed(issn_list)

    print("Verificando indexación en Scopus y WOS...")
    for issn in issn_list:
      appears_international = ''
      appears_scopus = check_scopus_indexing(issn)
      appears_wos = check_mjl_indexed(issn)
      if appears_scopus or appears_wos:
        appears_international = 'X'
      results.append([appears_scopus, appears_wos, appears_international])
      time.sleep(0.1)  # Avoid API rate limits

    new_columns = ['Scopus', 'WOS', 'Internacional']
    df_results[new_columns] = results
    total_row = pd.DataFrame([{
        'Artículos Publicados': 'Total',
        'Publindex': ((df_results['Publindex'] == 'X') & (df_results['Internacional'] == '')).sum(),
        'Scopus': (df_results['Scopus'] == 'X').sum(),
        'WOS': (df_results['WOS'] == 'X').sum(),
        'Internacional': (df_results['Internacional'] == 'X').sum()
    }])
    df_results = pd.concat([df_results, total_row], ignore_index=True)
    df_results.index = df_results.index + 1
    print("Resultados hallados: ")
    display(df_results)
    write_results(df_results, codes.iloc[i], file_id)
    i += 1
    break


#Extract ISSN from the Gruplac url given the starting and ending year
def issn_list_from_gruplac(url):
  start_year = 2019
  end_year = 2024
  page = requests.get(url)

  soup = BeautifulSoup(page.content, "html.parser")

  #Array to collect resulting articles
  articles = []
  issn_list = []
  tables = soup.find_all("table")
  articles_table = None

  #Goes through all the tables finding checking the publication year
  for table in tables:
    header_cell = table.find("td", class_="celdaEncabezado")
    if header_cell and "Artículos publicados" in header_cell.text:
      articles_table = table
      break

  for row in articles_table.find_all("tr"):
    if len(row.find_all("td")) >= 2:
      content_cell = row.find_all("td")[1]
      text = content_cell.get_text(separator = " ", strip=True)
      year_match = re.search(r'ISSN:\s+(?:\d{4}-\d{3}[\dxX]|0),?\s+((19|20)\d{2})\s+', text)
      if year_match:
        #print("\nYear extracted: " + str(year_match.group(1)) + " " + text)
        art_year = int(year_match.group(1))
        if start_year <= art_year <= end_year:
          # Extract ISSN
          issn_match = re.search(r'ISSN:\s*(\d{4}-\d{3}[0-9X])', text, re.IGNORECASE)
          if issn_match:
            issn = issn_match.group(1)
            issn_list.append(issn)
            articles.append(text)

    #Print results for debugging
  #i = 1
  #print(f"\nArtículos entre {start_year} y {end_year}:")
  #for article in articles:
    #print(f"{i}. {article}")
    #i += 1

  return issn_list, articles

#Acceses the Publindex web page and looks up the existance of the ISSN obtained from the list
def check_publindex_indexed(issn_list):

  options = Options()
  options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36")
  if "--headless" in options.arguments:
      options.arguments.remove("--headless")
  #<span class="ui-button-text ui-clickable">ui-btn</span>

  driver = gs.Chrome(options=options)
  driver.set_page_load_timeout(30)
  try:
    driver.get("https://scienti.minciencias.gov.co/publindex/#/revistasPublindex/buscador")
  except TimeoutException:
    print("Página de PUBLINDEX no responde")
    driver.quit()
    return

  #Screenshot display for debugging
  #driver.save_screenshot("screenshot.png")
  #display(Image("screenshot.png"))

  print("Conexión con PUBLINDEX exitosa")

  # Initialize wait
  wait = WebDriverWait(driver, 10)

  #Click the add search field button

  button = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[span[contains(@class, "fa-plus")]]')))
  button.click()

  dropdown_trigger = wait.until(EC.element_to_be_clickable(
      (By.CSS_SELECTOR, 'p-dropdown[formcontrolname="campo"] .ui-dropdown-trigger')
  ))

  dropdown_trigger.click()

  #Click on the ISSN dropdown option
  issn_option = wait.until(EC.element_to_be_clickable((
      By.XPATH, '//li[@role="option" and @aria-label="ISSN"]'
  )))
  issn_option.click()

  #Wait until the value text input is visible
  input_valor = wait.until(EC.visibility_of_element_located((
      By.CSS_SELECTOR, 'input[formcontrolname="valor"]'
  )))

  #Array to store the results of the searches
  results =[]

  for issn in issn_list:
    if issn in publindex_found:
      results.append(publindex_found[issn])
      #print("ISSN: " + issn + " ya lo había encontrado!")
      continue

    #Enter the issn keys
    input_valor.clear()
    input_valor.send_keys(issn)

    #Click on the search button
    buscar_btn = wait.until(EC.element_to_be_clickable((
        By.XPATH, '//button[span[contains(text(), "Buscar")]]'
    )))
    buscar_btn.click()

    #Wait until page has fully loaded
    wait.until(EC.invisibility_of_element_located((By.CSS_SELECTOR, '.ui-blockui.ui-widget-overlay')))

    #Check if the issn returned results

    try:
        fila_resultado = WebDriverWait(driver, 1).until(EC.presence_of_element_located((
            By.CSS_SELECTOR, 'tbody.ui-table-tbody tr.ng-star-inserted'
        )))

        columnas = fila_resultado.find_elements(By.TAG_NAME, "td")
        #print("Revista encontrada:")
        #print("  ISSN:", columnas[0].text.strip())
        #print("  Título:", columnas[1].text.strip())
        #print("  Área:", columnas[2].text.strip())
        results.append('X')
        publindex_found[issn] = 'X'

    except TimeoutException:
      try:
        WebDriverWait(driver, 1).until(EC.presence_of_element_located((By.XPATH, '//span[contains(@class, "ui-messages-detail") and contains(text(), "No se encontraron revistas")]')))
        #print("No se encontró ninguna revista con ese ISSN.")
        results.append('')
        publindex_found[issn] = ''

      except TimeoutException:
        print("Error de búsqueda")


  driver.quit()

  return results

#Checks the existance of a given ISSN in the scopus database via ita API
def check_scopus_indexing(issn):
    url = f"https://api.elsevier.com/content/serial/title?issn={issn}"
    headers = {
        "Accept": "application/json",
        "X-ELS-APIKey": 'your_scopus_api_key'
    }
    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        data = response.json()
        if "serial-metadata-response" in data:
          if 'error' in data['serial-metadata-response']:
            return ''
          elif 'entry' in data['serial-metadata-response']:
            return 'X'

#Checks a given issn in the WOS indexing base
def check_wos_indexing(issn):
  issn = str(issn)
  endpoint_url = "https://api.clarivate.com/apis/wos-starter/v1/journals"

  headers = {
    "X-ApiKey": 'your_wos_api_key',
    "Accept": 'application/json'
  }

  params = {
      "issn" : issn
  }

  response = requests.get(endpoint_url, headers=headers, params=params)

  if response.status_code == 200:
      data = response.json()
      if data.get('hits'):
        return 'X'
      else:
        return ''

  return ''



#Compares the issn with the MJL csv files
def check_mjl_indexed(issn):
  global mjl_df

  if(issn in mjl_df['ISSN'].values) or (issn in mjl_df['eISSN'].values):
    return 'X'
  else:
    return ''


#Writes the results into a new sheet of the excel file
def write_results(df, code, file_id):
  from openpyxl import load_workbook

  file_path = '/content/Grupos.xlsx'

  # Load the workbook and select the sheet
  book = load_workbook(file_path)

  # Check if the sheet exists
  if code in book.sheetnames:
    # Remove the existing sheet if it exists
    book.remove(book[code])

  # Create ExcelWriter with the loaded workbook
  with pd.ExcelWriter(file_path, mode = 'a', if_sheet_exists='replace') as writer:
      # writing to the 'Employee' sheet
      df.to_excel(writer, sheet_name=code, index=False)

  media = MediaFileUpload(file_path, mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
  updated_file = drive_service.files().update(fileId= file_id, media_body=media).execute()

"""CELDA DE EJECUCIÓN: (EJECUTAR PARA CADA ARCHIVO DE EXCEL)
1. Ejecutar la celda, la consola solicitará el URL del archivo de Excel de los grupos de investigación
*Asegurate de ingresar el link completo, incluyendo la cabecera _"https://"_

**Asegurate que esté autorizado la edición por parte de cualquier usuario con el link:
_"Cualquier usuario de Internet con el enlace puede editarlo"_
"""

excel_url = input("Ingrese el URL del Excel de Grupos de Investigación: ")
search_indexed_journals(excel_url)
